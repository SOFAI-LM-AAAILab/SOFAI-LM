{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SOFAI (Slow and Fast AI) Sampling Strategy in Mellea\n",
                "\n",
                "This notebook demonstrates the **SOFAI sampling strategy** in Mellea, specifically applied to the **Code Debugging** domain. SOFAI implements a dual-solver approach inspired by cognitive psychology's System 1 (fast) and System 2 (slow) thinking.\n",
                "\n",
                "## What is SOFAI?\n",
                "\n",
                "SOFAI (Slow and Fast AI) is a sampling strategy that uses two LLM solvers:\n",
                "\n",
                "1. **S1 Solver (Fast Model)**: A smaller, faster model that iteratively attempts to solve the problem with feedback-based repair\n",
                "2. **S2 Solver (Slow Model)**: A larger, more capable model that is called once when S1 fails to produce a valid solution\n",
                "\n",
                "This approach balances **cost efficiency** (using cheaper models when possible) with **quality** (escalating to more powerful models when needed)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SOFAI Flow Diagram\n",
                "\n",
                "```\n",
                "┌─────────────────────────────────────────────────────────┐\n",
                "│                    PHASE 1: S1 Solver                   │\n",
                "├─────────────────────────────────────────────────────────┤\n",
                "│                                                         │\n",
                "│  ┌─────────────┐    ┌──────────────┐    ┌───────────┐  │\n",
                "│  │  Generate   │───►│   Validate   │───►│  Success? │  │\n",
                "│  │  Solution   │    │  vs. Reqs    │    │           │  │\n",
                "│  └─────────────┘    └──────────────┘    └─────┬─────┘  │\n",
                "│         ▲                                     │        │\n",
                "│         │              Yes ◄──────────────────┤        │\n",
                "│         │                                     │ No     │\n",
                "│  ┌──────┴──────┐                              ▼        │\n",
                "│  │   Repair    │◄───────── Budget left? ◄────────      │\n",
                "│  │  Feedback   │               │                       │\n",
                "│  └─────────────┘               │ No                    │\n",
                "│                                ▼                       │\n",
                "└────────────────────────────────┼───────────────────────┘\n",
                "                                 │\n",
                "┌────────────────────────────────┼───────────────────────┐\n",
                "│                    PHASE 2: S2 Solver                   │\n",
                "├────────────────────────────────┼───────────────────────┤\n",
                "│                                ▼                       │\n",
                "│  ┌─────────────────────────────────────────────────┐   │\n",
                "│  │  Prepare context based on s2_solver_mode:       │   │\n",
                "│  │  • fresh_start: Original prompt only            │   │\n",
                "│  │  • continue_chat: Original + S1 history         │   │\n",
                "│  │  • best_attempt: Best S1 result + feedback      │   │\n",
                "│  └─────────────────────────────────────────────────┘   │\n",
                "│                                │                       │\n",
                "│                                ▼                       │\n",
                "│  ┌─────────────┐    ┌──────────────┐    ┌───────────┐  │\n",
                "│  │  Generate   │───►│   Validate   │───►│  Return   │  │\n",
                "│  │  Solution   │    │  vs. Reqs    │    │  Result   │  │\n",
                "│  └─────────────┘    └──────────────┘    └───────────┘  │\n",
                "│                                                         │\n",
                "└─────────────────────────────────────────────────────────┘\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "First, let's install Mellea from source by cloning the repository."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "skip-execution"
                ]
            },
            "outputs": [],
            "source": [
                "# Install Ollama (for Colab)\n",
                "!curl -fsSL https://ollama.com/install.sh | sh > /dev/null\n",
                "!nohup ollama serve >/dev/null 2>&1 &\n",
                "\n",
                "# Wait for Ollama to start\n",
                "import time\n",
                "time.sleep(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "skip-execution"
                ]
            },
            "outputs": [],
            "source": [
                "# Install Mellea from source (clone and install)\n",
                "!git clone https://github.com/generative-computing/mellea.git /tmp/mellea\n",
                "!cd /tmp/mellea && pip install -e \".[all]\" -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "skip-execution"
                ]
            },
            "outputs": [],
            "source": [
                "# Pull the required models\n",
                "!ollama pull phi:2.7b\n",
                "!ollama pull llama3.2:3b"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "\n",
                "import mellea\n",
                "from mellea.backends.ollama import OllamaModelBackend\n",
                "from mellea.core import FancyLogger\n",
                "from mellea.stdlib.context import ChatContext\n",
                "from mellea.stdlib.requirements import ValidationResult, req\n",
                "from mellea.stdlib.sampling import SOFAISamplingStrategy\n",
                "\n",
                "# Set logging level to see SOFAI's progress\n",
                "FancyLogger.get_logger().setLevel(logging.INFO)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SOFAISamplingStrategy Parameters\n",
                "\n",
                "Here are the key parameters for configuring SOFAI:\n",
                "\n",
                "| Parameter | Type | Description |\n",
                "|-----------|------|-------------|\n",
                "| `s1_solver_backend` | `Backend` | **Required**. Backend for the fast S1 solver (e.g., smaller model) |\n",
                "| `s2_solver_backend` | `Backend` | **Required**. Backend for the slow S2 solver (e.g., larger model) |\n",
                "| `s2_solver_mode` | `str` | How S2 receives context. Options: `\"fresh_start\"`, `\"continue_chat\"`, `\"best_attempt\"` |\n",
                "| `loop_budget` | `int` | Maximum attempts for S1 before escalating to S2 (default: 3) |\n",
                "| `judge_backend` | `Backend` | Optional third backend for LLM-as-Judge validation |\n",
                "| `feedback_strategy` | `str` | Feedback detail level: `\"simple\"`, `\"first_error\"`, `\"all_errors\"` |\n",
                "\n",
                "### S2 Solver Modes Explained\n",
                "\n",
                "| Mode | Description | Best For |\n",
                "|------|-------------|----------|\n",
                "| `fresh_start` | S2 gets only the original prompt (clean slate) | Independent problem solving |\n",
                "| `continue_chat` | S2 gets original prompt + entire S1 conversation history | Learning from S1's attempts |\n",
                "| `best_attempt` | S2 gets original prompt + best S1 attempt + feedback summary | Focused improvement on best solution |\n",
                "\n",
                "### Feedback Strategies (for LLM-as-Judge)\n",
                "\n",
                "| Strategy | Description |\n",
                "|----------|-------------|\n",
                "| `simple` | Binary yes/no validation, no detailed feedback |\n",
                "| `first_error` | Reports only the first mistake found with detailed feedback |\n",
                "| `all_errors` | Comprehensive feedback about all mistakes found |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Code Debugging Domain\n",
                "\n",
                "Now let's apply SOFAI to **code debugging** - fixing buggy Python code. This is an excellent domain for SOFAI because:\n",
                "\n",
                "1. We can **programmatically validate** code by executing it and checking outputs\n",
                "2. Smaller models can often fix simple bugs, but complex logic errors may need larger models\n",
                "3. Feedback can be very specific (e.g., \"Expected 5, got 4 for input [1,2,3,4,5]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example 1: Two Sum with Custom Validator\n",
                "\n",
                "A classic coding problem with a subtle bug. We'll use a **custom validation function** that executes the code and checks test cases."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Buggy code to fix\n",
                "buggy_two_sum = '''\n",
                "def two_sum(nums, target):\n",
                "    \"\"\"Return indices of two numbers that add up to target.\"\"\"\n",
                "    seen = {}\n",
                "    for i, num in enumerate(nums):\n",
                "        complement = target - num\n",
                "        if complement in seen:\n",
                "            return [i, seen[complement]]  # BUG: indices are swapped!\n",
                "        seen[num] = i\n",
                "    return []\n",
                "'''\n",
                "\n",
                "# Test cases for validation\n",
                "two_sum_tests = [\n",
                "    {\"input\": {\"nums\": [2, 7, 11, 15], \"target\": 9}, \"expected\": [0, 1]},\n",
                "    {\"input\": {\"nums\": [3, 2, 4], \"target\": 6}, \"expected\": [1, 2]},\n",
                "    {\"input\": {\"nums\": [3, 3], \"target\": 6}, \"expected\": [0, 1]},\n",
                "]\n",
                "\n",
                "print(\"Buggy code:\")\n",
                "print(buggy_two_sum)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_python_code(response: str) -> str:\n",
                "    \"\"\"Extract Python code from LLM response (handles markdown code blocks).\"\"\"\n",
                "    response = response.strip()\n",
                "    \n",
                "    # Try to extract from markdown code blocks\n",
                "    if \"```python\" in response:\n",
                "        start = response.find(\"```python\") + len(\"```python\")\n",
                "        end = response.find(\"```\", start)\n",
                "        if end > start:\n",
                "            return response[start:end].strip()\n",
                "    elif \"```\" in response:\n",
                "        start = response.find(\"```\") + 3\n",
                "        end = response.find(\"```\", start)\n",
                "        if end > start:\n",
                "            return response[start:end].strip()\n",
                "    \n",
                "    # If no code blocks, try to find def statement\n",
                "    if \"def \" in response:\n",
                "        lines = response.split(\"\\n\")\n",
                "        code_lines = []\n",
                "        in_function = False\n",
                "        for line in lines:\n",
                "            if line.strip().startswith(\"def \"):\n",
                "                in_function = True\n",
                "            if in_function:\n",
                "                code_lines.append(line)\n",
                "        return \"\\n\".join(code_lines)\n",
                "    \n",
                "    return response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_code_validator(test_cases: list, function_name: str):\n",
                "    \"\"\"Create a validator that tests the fixed code against test cases.\"\"\"\n",
                "    \n",
                "    def validate_code(ctx) -> ValidationResult:\n",
                "        output = ctx.last_output()\n",
                "        if output is None:\n",
                "            return ValidationResult(False, reason=\"No output found.\")\n",
                "        \n",
                "        # Extract code from response\n",
                "        fixed_code = extract_python_code(str(output.value))\n",
                "        if not fixed_code or \"def \" not in fixed_code:\n",
                "            return ValidationResult(\n",
                "                False, \n",
                "                reason=f\"Could not find valid Python function. Please provide the complete fixed function starting with 'def {function_name}'.\"\n",
                "            )\n",
                "        \n",
                "        # Execute and test\n",
                "        errors = []\n",
                "        try:\n",
                "            namespace = {}\n",
                "            exec(fixed_code, namespace)  # noqa: S102\n",
                "            \n",
                "            if function_name not in namespace:\n",
                "                return ValidationResult(\n",
                "                    False, \n",
                "                    reason=f\"Function '{function_name}' not found in code. Please provide the complete function.\"\n",
                "                )\n",
                "            \n",
                "            func = namespace[function_name]\n",
                "            \n",
                "            for i, test in enumerate(test_cases):\n",
                "                try:\n",
                "                    result = func(**test[\"input\"])\n",
                "                    expected = test[\"expected\"]\n",
                "                    \n",
                "                    if result != expected:\n",
                "                        errors.append(\n",
                "                            f\"Test {i+1} FAILED: {function_name}({test['input']}) \"\n",
                "                            f\"returned {result}, expected {expected}\"\n",
                "                        )\n",
                "                except Exception as e:\n",
                "                    errors.append(f\"Test {i+1} raised exception: {type(e).__name__}: {e}\")\n",
                "                    \n",
                "        except SyntaxError as e:\n",
                "            return ValidationResult(False, reason=f\"Syntax error in code: {e}\")\n",
                "        except Exception as e:\n",
                "            return ValidationResult(False, reason=f\"Error executing code: {type(e).__name__}: {e}\")\n",
                "        \n",
                "        if errors:\n",
                "            return ValidationResult(False, reason=\" | \".join(errors))\n",
                "        \n",
                "        return ValidationResult(True, reason=\"All test cases passed!\")\n",
                "    \n",
                "    return validate_code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set up backends\n",
                "s1_solver = OllamaModelBackend(model_id=\"phi:2.7b\")\n",
                "s2_solver = OllamaModelBackend(model_id=\"llama3.2:3b\")\n",
                "\n",
                "# Create SOFAI strategy with custom validator (no judge_backend)\n",
                "sofai_strategy = SOFAISamplingStrategy(\n",
                "    s1_solver_backend=s1_solver,\n",
                "    s2_solver_backend=s2_solver,\n",
                "    s2_solver_mode=\"best_attempt\",\n",
                "    loop_budget=3,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the debugging prompt\n",
                "debug_prompt_two_sum = f\"\"\"\n",
                "The following Python function has a bug. Fix it.\n",
                "\n",
                "```python\n",
                "{buggy_two_sum}\n",
                "```\n",
                "\n",
                "Test Cases:\n",
                "- two_sum([2, 7, 11, 15], 9) should return [0, 1] (indices of 2 and 7)\n",
                "- two_sum([3, 2, 4], 6) should return [1, 2] (indices of 2 and 4)\n",
                "- two_sum([3, 3], 6) should return [0, 1]\n",
                "\n",
                "Provide the complete fixed function.\n",
                "\"\"\"\n",
                "\n",
                "# Create requirement with custom validator\n",
                "two_sum_requirement = [\n",
                "    req(\n",
                "        description=\"The fixed code must pass all test cases with correct output.\",\n",
                "        validation_fn=create_code_validator(two_sum_tests, \"two_sum\")\n",
                "    )\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run SOFAI debugging\n",
                "print(\"=\" * 60)\n",
                "print(\"SOFAI Code Debugging: Two Sum (Custom Validator)\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "m = mellea.MelleaSession(backend=s1_solver, ctx=ChatContext())\n",
                "\n",
                "result = m.instruct(\n",
                "    debug_prompt_two_sum,\n",
                "    requirements=two_sum_requirement,\n",
                "    strategy=sofai_strategy,\n",
                "    return_sampling_results=True,\n",
                "    model_options={\"temperature\": 0.2},\n",
                ")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"SUCCESS: {result.success}\")\n",
                "print(f\"Total attempts: {len(result.sample_generations)}\")\n",
                "print(f\"{'='*60}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display detailed results for Example 1\n",
                "for i, (gen, val_list) in enumerate(zip(result.sample_generations, result.sample_validations)):\n",
                "    solver_name = \"S1 Solver\" if i < sofai_strategy.loop_budget else \"S2 Solver\"\n",
                "    status = \"✓ PASS\" if all(v[1].as_bool() for v in val_list) else \"✗ FAIL\"\n",
                "    \n",
                "    print(f\"\\n--- Attempt {i + 1} ({solver_name}) [{status}] ---\")\n",
                "    print(f\"Output:\\n{gen.value[:500]}...\" if len(str(gen.value)) > 500 else f\"Output:\\n{gen.value}\")\n",
                "    \n",
                "    for req_obj, val_result in val_list:\n",
                "        print(f\"Validation: {val_result.reason}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Example 2: FizzBuzz with LLM-as-Judge (No Custom Validator)\n",
                "\n",
                "This example demonstrates using **LLM-as-Judge** for validation instead of a custom validation function. This is useful when:\n",
                "- You don't have programmatic test cases\n",
                "- The correctness criterion is subjective or complex to encode\n",
                "- You want quick prototyping without writing validators\n",
                "\n",
                "We configure SOFAI with a `judge_backend` and set the `feedback_strategy` to control how detailed the feedback is."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Buggy FizzBuzz implementation\n",
                "buggy_fizzbuzz = '''\n",
                "def fizzbuzz(n):\n",
                "    \"\"\"Return a list of FizzBuzz results from 1 to n.\n",
                "    \n",
                "    Rules:\n",
                "    - Return 'Fizz' for multiples of 3\n",
                "    - Return 'Buzz' for multiples of 5\n",
                "    - Return 'FizzBuzz' for multiples of both 3 and 5\n",
                "    - Return the number as string otherwise\n",
                "    \"\"\"\n",
                "    result = []\n",
                "    for i in range(1, n + 1):\n",
                "        if i % 3 == 0:\n",
                "            result.append(\"Fizz\")\n",
                "        elif i % 5 == 0:\n",
                "            result.append(\"Buzz\")\n",
                "        # BUG: Missing FizzBuzz case for multiples of both 3 and 5!\n",
                "        else:\n",
                "            result.append(str(i))\n",
                "    return result\n",
                "'''\n",
                "\n",
                "print(\"Buggy FizzBuzz code:\")\n",
                "print(buggy_fizzbuzz)\n",
                "\n",
                "# Show what the buggy code produces\n",
                "exec(buggy_fizzbuzz)  # noqa: S102\n",
                "print(\"\\nBuggy output for fizzbuzz(15):\")\n",
                "print(fizzbuzz(15))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set up SOFAI with LLM-as-Judge\n",
                "# We use a third model as the judge\n",
                "judge_backend = OllamaModelBackend(model_id=\"llama3.2:3b\")\n",
                "\n",
                "# Create SOFAI strategy WITH judge_backend and feedback_strategy\n",
                "sofai_strategy_llm_judge = SOFAISamplingStrategy(\n",
                "    s1_solver_backend=s1_solver,\n",
                "    s2_solver_backend=s2_solver,\n",
                "    s2_solver_mode=\"fresh_start\",\n",
                "    loop_budget=3,\n",
                "    # LLM-as-Judge configuration:\n",
                "    judge_backend=judge_backend,  # Third model for validation\n",
                "    feedback_strategy=\"all_errors\",  # Options: \"simple\", \"first_error\", \"all_errors\"\n",
                ")\n",
                "\n",
                "print(\"SOFAI configured with LLM-as-Judge:\")\n",
                "print(f\"  • S1 Solver: phi:2.7b\")\n",
                "print(f\"  • S2 Solver: llama3.2:3b\")\n",
                "print(f\"  • Judge: llama3.2:3b\")\n",
                "print(f\"  • Feedback Strategy: all_errors\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the debugging prompt for FizzBuzz\n",
                "debug_prompt_fizzbuzz = f\"\"\"\n",
                "The following Python function has a bug in its logic. Fix it.\n",
                "\n",
                "```python\n",
                "{buggy_fizzbuzz}\n",
                "```\n",
                "\n",
                "The function should:\n",
                "1. Return 'FizzBuzz' for numbers divisible by BOTH 3 and 5 (like 15, 30)\n",
                "2. Return 'Fizz' for numbers divisible by 3 only\n",
                "3. Return 'Buzz' for numbers divisible by 5 only\n",
                "4. Return the number as a string otherwise\n",
                "\n",
                "For example, fizzbuzz(15) should produce:\n",
                "['1', '2', 'Fizz', '4', 'Buzz', 'Fizz', '7', '8', 'Fizz', 'Buzz', '11', 'Fizz', '13', '14', 'FizzBuzz']\n",
                "\n",
                "Note that position 15 should be 'FizzBuzz', not 'Fizz'.\n",
                "\n",
                "Provide the complete fixed function.\n",
                "\"\"\"\n",
                "\n",
                "# Create requirement WITHOUT validation_fn - will use LLM-as-Judge\n",
                "fizzbuzz_requirement = [\n",
                "    req(\n",
                "        description=\"The fixed code must correctly handle the FizzBuzz logic: return 'FizzBuzz' for multiples of both 3 and 5, 'Fizz' for multiples of 3 only, 'Buzz' for multiples of 5 only, and the number as string otherwise.\"\n",
                "    )\n",
                "]\n",
                "\n",
                "print(\"Requirement (no validation_fn, will use LLM-as-Judge):\")\n",
                "print(f\"  {fizzbuzz_requirement[0].description}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run SOFAI debugging with LLM-as-Judge\n",
                "print(\"=\" * 60)\n",
                "print(\"SOFAI Code Debugging: FizzBuzz (LLM-as-Judge)\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "m2 = mellea.MelleaSession(backend=s1_solver, ctx=ChatContext())\n",
                "\n",
                "result2 = m2.instruct(\n",
                "    debug_prompt_fizzbuzz,\n",
                "    requirements=fizzbuzz_requirement,\n",
                "    strategy=sofai_strategy_llm_judge,\n",
                "    return_sampling_results=True,\n",
                "    model_options={\"temperature\": 0.2},\n",
                ")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"SUCCESS: {result2.success}\")\n",
                "print(f\"Total attempts: {len(result2.sample_generations)}\")\n",
                "print(f\"{'='*60}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display detailed results showing LLM-as-Judge feedback\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"LLM-as-Judge Validation Details\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for i, (gen, val_list) in enumerate(zip(result2.sample_generations, result2.sample_validations)):\n",
                "    solver_name = \"S1 Solver\" if i < sofai_strategy_llm_judge.loop_budget else \"S2 Solver\"\n",
                "    status = \"✓ PASS\" if all(v[1].as_bool() for v in val_list) else \"✗ FAIL\"\n",
                "    \n",
                "    print(f\"\\n{'─'*60}\")\n",
                "    print(f\"Attempt {i + 1} ({solver_name}) [{status}]\")\n",
                "    print(f\"{'─'*60}\")\n",
                "    \n",
                "    # Show the generated code\n",
                "    code = extract_python_code(str(gen.value))\n",
                "    print(f\"\\nGenerated Code:\")\n",
                "    print(code[:400] + \"...\" if len(code) > 400 else code)\n",
                "    \n",
                "    # Show LLM-as-Judge feedback\n",
                "    print(f\"\\nLLM-as-Judge Feedback:\")\n",
                "    for req_obj, val_result in val_list:\n",
                "        print(f\"  Valid: {val_result.as_bool()}\")\n",
                "        print(f\"  Reason: {val_result.reason}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify the final result by running the fixed code\n",
                "if result2.success:\n",
                "    final_code = extract_python_code(str(result2.result.value))\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"FINAL FIXED CODE\")\n",
                "    print(\"=\" * 60)\n",
                "    print(final_code)\n",
                "    \n",
                "    # Actually run the fixed code to verify\n",
                "    print(\"\\n\" + \"=\" * 60)\n",
                "    print(\"VERIFICATION: Running fizzbuzz(15)\")\n",
                "    print(\"=\" * 60)\n",
                "    try:\n",
                "        namespace = {}\n",
                "        exec(final_code, namespace)  # noqa: S102\n",
                "        if \"fizzbuzz\" in namespace:\n",
                "            output = namespace[\"fizzbuzz\"](15)\n",
                "            print(f\"Output: {output}\")\n",
                "            expected = ['1', '2', 'Fizz', '4', 'Buzz', 'Fizz', '7', '8', 'Fizz', 'Buzz', '11', 'Fizz', '13', '14', 'FizzBuzz']\n",
                "            print(f\"Expected: {expected}\")\n",
                "            print(f\"Match: {'✓ YES' if output == expected else '✗ NO'}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error running code: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Comparison: Custom Validator vs LLM-as-Judge\n",
                "\n",
                "| Aspect | Custom Validator | LLM-as-Judge |\n",
                "|--------|-----------------|---------------|\n",
                "| **Setup** | Requires writing test code | Just write a description |\n",
                "| **Accuracy** | Deterministic, 100% reliable | Can be inconsistent |\n",
                "| **Feedback** | Specific error messages | Natural language explanation |\n",
                "| **Speed** | Fast | Slower (LLM call) |\n",
                "| **Cost** | Free | Extra LLM calls |\n",
                "| **Best For** | Algorithmic problems | Subjective quality, code style |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Understanding SOFAI Behavior\n",
                "\n",
                "Let's analyze both runs to see how SOFAI behaved."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_sofai_run(result, strategy, name):\n",
                "    \"\"\"Analyze and display SOFAI execution details.\"\"\"\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Analysis: {name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    total_attempts = len(result.sample_generations)\n",
                "    s1_attempts = min(strategy.loop_budget, total_attempts)\n",
                "    s2_used = total_attempts > s1_attempts\n",
                "    \n",
                "    print(f\"\\nConfiguration:\")\n",
                "    print(f\"  • S1 Loop Budget: {strategy.loop_budget}\")\n",
                "    print(f\"  • S2 Mode: {strategy.s2_solver_mode}\")\n",
                "    print(f\"  • Judge Backend: {'Yes' if strategy.judge_backend else 'No (custom validator)'}\")\n",
                "    if strategy.judge_backend:\n",
                "        print(f\"  • Feedback Strategy: {strategy.feedback_strategy}\")\n",
                "    \n",
                "    print(f\"\\nExecution:\")\n",
                "    print(f\"  • S1 Attempts: {s1_attempts}\")\n",
                "    print(f\"  • S2 Used: {'Yes' if s2_used else 'No'}\")\n",
                "    print(f\"  • Final Result: {'✓ SUCCESS' if result.success else '✗ FAILED'}\")\n",
                "    \n",
                "    print(f\"\\nAttempt Details:\")\n",
                "    for i, val_list in enumerate(result.sample_validations):\n",
                "        solver = \"S1\" if i < s1_attempts else \"S2\"\n",
                "        passed = sum(1 for _, v in val_list if v.as_bool())\n",
                "        total = len(val_list)\n",
                "        status = \"✓\" if passed == total else \"✗\"\n",
                "        print(f\"  {i+1}. [{solver}] {status} Passed {passed}/{total} requirements\")\n",
                "\n",
                "# Analyze both runs\n",
                "analyze_sofai_run(result, sofai_strategy, \"Two Sum (Custom Validator)\")\n",
                "analyze_sofai_run(result2, sofai_strategy_llm_judge, \"FizzBuzz (LLM-as-Judge)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "### When to Use SOFAI\n",
                "\n",
                "SOFAI is ideal when:\n",
                "1. **Cost matters**: You want to use cheaper/faster models when possible\n",
                "2. **Quality matters**: You need a fallback to more capable models for hard cases\n",
                "3. **Feedback is informative**: Validators can provide specific error messages\n",
                "4. **Iterative improvement is possible**: The problem allows incremental fixes\n",
                "\n",
                "### SOFAI vs RejectionSampling\n",
                "\n",
                "| Aspect | RejectionSampling | SOFAI |\n",
                "|--------|------------------|-------|\n",
                "| Models | Single model | Two models (fast + slow) |\n",
                "| Feedback | Simple retry | Targeted repair messages |\n",
                "| Cost | Higher (same model for all) | Lower (cheap model first) |\n",
                "| Complexity | Simple | More sophisticated |\n",
                "\n",
                "### Best Practices\n",
                "\n",
                "1. **Use custom validators when possible**: Programmatic tests are faster and more reliable\n",
                "2. **Use LLM-as-Judge for subjective criteria**: Code style, documentation quality, etc.\n",
                "3. **Choose `feedback_strategy` wisely**:\n",
                "   - `simple`: Fast, but no guidance for fixing\n",
                "   - `first_error`: Good balance of speed and guidance\n",
                "   - `all_errors`: Most informative, but slower\n",
                "4. **Choose S2 mode based on problem type**:\n",
                "   - `best_attempt`: When S1's attempts contain useful partial solutions\n",
                "   - `continue_chat`: When the conversation context helps\n",
                "   - `fresh_start`: When S1's attempts might confuse S2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Further Reading\n",
                "\n",
                "- [Mellea Documentation](https://mellea.ai/)\n",
                "- [Mellea Tutorial](https://github.com/generative-computing/mellea/blob/main/docs/tutorial.md)\n",
                "- [SOFAI Source Code](https://github.com/generative-computing/mellea/blob/main/mellea/stdlib/sampling/sofai.py)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}